# Vercel Environment Variables for Synapse
# Copy these to your Vercel project environment variables

# Backend Configuration
PORT=9000
HOST=0.0.0.0
CORS_ORIGINS=*

# LLM Configuration (if using external services)
LLAMA_URL=https://your-llm-service.com/v1/chat/completions
LLAMA_MODEL=qwen2.5-3b-instruct-q4_k_m

# Embedding Model
EMB_MODEL=BAAI/bge-base-en-v1.5

# Context Size
CTX_SIZE=32768

# Guardian Configuration
GUARDIAN_ENABLED=false

# Note: For Vercel deployment, some features like local file storage
# and llama.cpp integration will not work. Consider using external
# services for these features.
